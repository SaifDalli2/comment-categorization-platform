# Prometheus Alerting Rules for Gateway Service
# File: prometheus/rules/gateway_alerts.yml

groups:
  - name: gateway.availability
    interval: 30s
    rules:
    - alert: GatewayDown
      expr: up{job="gateway"} == 0
      for: 1m
      labels:
        severity: critical
        component: gateway
        team: platform
      annotations:
        summary: "Gateway service is down"
        description: "Gateway service has been down for more than 1 minute. No requests can be processed."
        runbook_url: "https://runbooks.claude-analysis.com/gateway/down"
        dashboard_url: "https://grafana.claude-analysis.com/d/gateway-overview"

    - alert: GatewayHighErrorRate
      expr: |
        (
          rate(gateway_http_requests_total{status_code=~"5.."}[5m]) /
          rate(gateway_http_requests_total[5m])
        ) * 100 > 5
      for: 5m
      labels:
        severity: critical
        component: gateway
        team: platform
      annotations:
        summary: "Gateway error rate is high"
        description: "Gateway error rate is {{ $value | humanizePercentage }} over the last 5 minutes (threshold: 5%)"
        runbook_url: "https://runbooks.claude-analysis.com/gateway/high-error-rate"

    - alert: GatewayHighLatency
      expr: |
        histogram_quantile(0.95, 
          rate(gateway_http_request_duration_seconds_bucket[5m])
        ) > 1
      for: 5m
      labels:
        severity: warning
        component: gateway
        team: platform
      annotations:
        summary: "Gateway response time is high"
        description: "Gateway 95th percentile latency is {{ $value }}s over the last 5 minutes (threshold: 1s)"
        runbook_url: "https://runbooks.claude-analysis.com/gateway/high-latency"

  - name: gateway.capacity
    interval: 30s
    rules:
    - alert: GatewayHighRequestRate
      expr: rate(gateway_http_requests_total[1m]) > 1000
      for: 2m
      labels:
        severity: warning
        component: gateway
        team: platform
      annotations:
        summary: "Gateway request rate is very high"
        description: "Gateway is receiving {{ $value }} requests per second (threshold: 1000 req/s)"
        runbook_url: "https://runbooks.claude-analysis.com/gateway/high-request-rate"

    - alert: GatewayRateLimitingActive
      expr: rate(gateway_rate_limit_hits_total[5m]) > 10
      for: 2m
      labels:
        severity: warning
        component: gateway
        team: platform
      annotations:
        summary: "High rate limiting activity detected"
        description: "Rate limiting is blocking {{ $value }} requests per second"
        runbook_url: "https://runbooks.claude-analysis.com/gateway/rate-limiting"

  - name: gateway.services
    interval: 30s
    rules:
    - alert: ServiceUnhealthy
      expr: gateway_service_health_status == 0
      for: 2m
      labels:
        severity: warning
        component: gateway
        team: platform
        service_name: "{{ $labels.service_name }}"
      annotations:
        summary: "Backend service is unhealthy"
        description: "Service {{ $labels.service_name }} ({{ $labels.instance_id }}) has been unhealthy for more than 2 minutes"
        runbook_url: "https://runbooks.claude-analysis.com/services/unhealthy"

    - alert: CircuitBreakerOpen
      expr: gateway_circuit_breaker_state == 1
      for: 30s
      labels:
        severity: critical
        component: gateway
        team: platform
        service_name: "{{ $labels.service_name }}"
      annotations:
        summary: "Circuit breaker is open"
        description: "Circuit breaker for {{ $labels.service_name }} ({{ $labels.instance_id }}) is open, requests are being blocked"
        runbook_url: "https://runbooks.claude-analysis.com/gateway/circuit-breaker-open"

    - alert: ServiceHighErrorRate
      expr: |
        (
          rate(gateway_service_requests_total{status_code=~"5.."}[5m]) /
          rate(gateway_service_requests_total[5m])
        ) * 100 > 10
      for: 3m
      labels:
        severity: warning
        component: gateway
        team: platform
        service_name: "{{ $labels.service_name }}"
      annotations:
        summary: "High error rate from backend service"
        description: "Service {{ $labels.service_name }} has {{ $value | humanizePercentage }} error rate over the last 5 minutes"
        runbook_url: "https://runbooks.claude-analysis.com/services/high-error-rate"

  - name: gateway.authentication
    interval: 30s
    rules:
    - alert: AuthenticationFailureSpike
      expr: |
        rate(gateway_auth_requests_total{status="failure"}[5m]) > 5
      for: 3m
      labels:
        severity: warning
        component: gateway
        team: security
      annotations:
        summary: "High authentication failure rate"
        description: "Authentication failure rate is {{ $value }} failures per second over the last 5 minutes"
        runbook_url: "https://runbooks.claude-analysis.com/security/auth-failures"

    - alert: AuthTokenCacheLowHitRate
      expr: |
        (
          rate(gateway_auth_token_cache_hits_total[5m]) /
          (rate(gateway_auth_token_cache_hits_total[5m]) + rate(gateway_auth_token_cache_misses_total[5m]))
        ) * 100 < 80
      for: 5m
      labels:
        severity: info
        component: gateway
        team: platform
      annotations:
        summary: "Auth token cache hit rate is low"
        description: "Token cache hit rate is {{ $value | humanizePercentage }} (threshold: 80%)"

  - name: gateway.security
    interval: 30s
    rules:
    - alert: CORSViolationSpike
      expr: rate(gateway_cors_requests_total{status="blocked"}[5m]) > 1
      for: 2m
      labels:
        severity: warning
        component: gateway
        team: security
      annotations:
        summary: "High rate of CORS violations"
        description: "CORS is blocking {{ $value }} requests per second"
        runbook_url: "https://runbooks.claude-analysis.com/security/cors-violations"

    - alert: SuspiciousRequestPattern
      expr: |
        rate(gateway_http_requests_total{status_code="404"}[5m]) > 50
      for: 3m
      labels:
        severity: info
        component: gateway
        team: security
      annotations:
        summary: "High rate of 404 errors detected"
        description: "{{ $value }} 404 errors per second, possible scanning activity"

  - name: gateway.performance
    interval: 30s
    rules:
    - alert: GatewayMemoryUsageHigh
      expr: |
        (
          process_resident_memory_bytes{job="gateway"} / 
          (1024 * 1024 * 1024)
        ) > 1.5
      for: 5m
      labels:
        severity: warning
        component: gateway
        team: platform
      annotations:
        summary: "Gateway memory usage is high"
        description: "Gateway is using {{ $value }}GB of memory (threshold: 1.5GB)"

    - alert: GatewayCPUUsageHigh
      expr: rate(process_cpu_seconds_total{job="gateway"}[5m]) > 0.8
      for: 5m
      labels:
        severity: warning
        component: gateway
        team: platform
      annotations:
        summary: "Gateway CPU usage is high"
        description: "Gateway CPU usage is {{ $value | humanizePercentage }} (threshold: 80%)"

    - alert: ActiveConnectionsHigh
      expr: gateway_active_connections > 1000
      for: 3m
      labels:
        severity: warning
        component: gateway
        team: platform
      annotations:
        summary: "High number of active connections"
        description: "Gateway has {{ $value }} active connections (threshold: 1000)"

  - name: gateway.health-checks
    interval: 60s
    rules:
    - alert: HealthCheckFailures
      expr: |
        rate(gateway_service_health_checks_total{status="unhealthy"}[10m]) > 0.1
      for: 5m
      labels:
        severity: info
        component: gateway
        team: platform
        service_name: "{{ $labels.service_name }}"
      annotations:
        summary: "Frequent health check failures"
        description: "Service {{ $labels.service_name }} health checks are failing {{ $value }} times per second"

    - alert: HealthCheckTimeout
      expr: |
        histogram_quantile(0.95,
          rate(gateway_service_health_check_duration_seconds_bucket[5m])
        ) > 3
      for: 3m
      labels:
        severity: warning
        component: gateway
        team: platform
      annotations:
        summary: "Health check response time is high"
        description: "95th percentile health check duration is {{ $value }}s (threshold: 3s)"

  - name: gateway.business
    interval: 60s
    rules:
    - alert: CommentProcessingJobsStuck
      expr: |
        gateway_custom_comment_processing_jobs_total{status="processing"} > 100
      for: 10m
      labels:
        severity: warning
        component: gateway
        team: product
      annotations:
        summary: "High number of stuck comment processing jobs"
        description: "{{ $value }} comment processing jobs have been in processing state for over 10 minutes"

    - alert: NPSUploadFailures
      expr: |
        rate(gateway_service_requests_total{service_name="nps",status_code=~"5.."}[10m]) > 0.1
      for: 5m
      labels:
        severity: warning
        component: gateway
        team: product
      annotations:
        summary: "NPS upload failures detected"
        description: "NPS service is experiencing {{ $value }} failures per second"

# Alertmanager configuration for routing these alerts
alertmanager_config: |
  global:
    smtp_smarthost: 'smtp.gmail.com:587'
    smtp_from: 'alerts@claude-analysis.com'

  route:
    group_by: ['alertname', 'cluster', 'service']
    group_wait: 10s
    group_interval: 10s
    repeat_interval: 1h
    receiver: 'default'
    routes:
    - match:
        severity: critical
      receiver: 'critical-alerts'
      continue: true
    - match:
        team: security
      receiver: 'security-team'
    - match:
        team: platform
      receiver: 'platform-team'
    - match:
        team: product
      receiver: 'product-team'

  receivers:
  - name: 'default'
    slack_configs:
    - api_url: 'YOUR_SLACK_WEBHOOK_URL'
      channel: '#alerts'
      title: '{{ .GroupLabels.alertname }}'
      text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'

  - name: 'critical-alerts'
    slack_configs:
    - api_url: 'YOUR_SLACK_WEBHOOK_URL'
      channel: '#critical-alerts'
      title: '🚨 CRITICAL: {{ .GroupLabels.alertname }}'
      text: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ .Annotations.description }}{{ end }}'
    email_configs:
    - to: 'oncall@claude-analysis.com'
      subject: 'CRITICAL ALERT: {{ .GroupLabels.alertname }}'
              body: |
        {{ range .Alerts }}
        Alert: {{ .Annotations.summary }}
        Description: {{ .Annotations.description }}
        Runbook: {{ .Annotations.runbook_url }}
        Dashboard: {{ .Annotations.dashboard_url }}
        {{ end }}

  - name: 'security-team'
    slack_configs:
    - api_url: 'YOUR_SECURITY_SLACK_WEBHOOK_URL'
      channel: '#security-alerts'
      title: '🔒 Security Alert: {{ .GroupLabels.alertname }}'
      text: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ .Annotations.description }}{{ end }}'

  - name: 'platform-team'
    slack_configs:
    - api_url: 'YOUR_PLATFORM_SLACK_WEBHOOK_URL'
      channel: '#platform-alerts'
      title: '⚙️ Platform Alert: {{ .GroupLabels.alertname }}'
      text: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ .Annotations.description }}{{ end }}'

  - name: 'product-team'
    slack_configs:
    - api_url: 'YOUR_PRODUCT_SLACK_WEBHOOK_URL'
      channel: '#product-alerts'
      title: '📊 Product Alert: {{ .GroupLabels.alertname }}'
      text: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ .Annotations.description }}{{ end }}'

  inhibit_rules:
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'cluster', 'service']